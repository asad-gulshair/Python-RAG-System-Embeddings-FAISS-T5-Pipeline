# -*- coding: utf-8 -*-
"""rag system

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vzohu6kzDfylDROX5NIzzqwJs0xck6X5
"""

!pip install -q sentence-transformers faiss-cpu transformers accelerate tqdm

import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import torch
from tqdm.auto import tqdm

from google.colab import files

uploaded = files.upload()

filename = next(iter(uploaded))

with open(filename, 'r', encoding='utf-8') as f:
    docs = [line.strip() for line in f if line.strip()]

print("Loaded documents:", len(docs))
docs[:5]   # show first 5 documents

print("Loaded from file:", len(docs))

def chunk(text, max_chars=300):
    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]

chunks = []
for d in docs:
    chunks.extend(chunk(d))

print("Chunks created:", len(chunks))

embedder = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")
print("Embedding model ready")

embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)
embeddings = embeddings.astype("float32")

print("Embeddings shape:", embeddings.shape)

faiss.normalize_L2(embeddings)

dimension = embeddings.shape[1]
index = faiss.IndexFlatIP(dimension)
index.add(embeddings)

print("FAISS index built with", index.ntotal, "vectors")

def retrieve(query, top_k=3):
    q_emb = embedder.encode([query], convert_to_numpy=True).astype("float32")
    faiss.normalize_L2(q_emb)

    scores, ids = index.search(q_emb, top_k)
    results = [(chunks[i], float(scores[0][j])) for j, i in enumerate(ids[0])]
    return results

# Example
retrieve("How long do cats sleep?")

model_name = "google/flan-t5-small"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

generator = pipeline("text2text-generation", model=model, tokenizer=tokenizer, device=0 if device=="cuda" else -1)

print("Generator ready on", device)

def build_prompt(query, retrieved):
    context = "\n".join([f"- {c}" for c, s in retrieved])
    return f"""
Use ONLY the context to answer. If not found, say 'I don't know'.

Context:
{context}

Question: {query}
Answer:
""".strip()

def ask_rag(query, top_k=3):
    retrieved = retrieve(query, top_k)
    prompt = build_prompt(query, retrieved)
    answer = generator(prompt, max_length=120, do_sample=False)[0]["generated_text"]

    return {
        "query": query,
        "retrieved": [c for c, s in retrieved],
        "answer": answer
    }

# Test it
result = ask_rag("How fast can a cat run?")
result

ask_rag("What do cats use whiskers for?")

ask_rag("WHY SOME CATS ARE BLACK AND SOME WHITE?")

